{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52bed587-3e11-40b9-9d6c-8c2425408914",
   "metadata": {
    "id": "hXQd-PHxxN1Q"
   },
   "source": [
    "# Lab Five: Wide and Deep Networks\n",
    "\n",
    "***Md Mahfuzur Rahman, Will Schneider, Nik Zelenikovski***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcc1cb0-4710-4141-947f-e01e51fe82a0",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d472a1ce-d562-40dd-a911-fc030f0fcf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23710 entries, 0 to 23709\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   work_year           23710 non-null  int64 \n",
      " 1   experience_level    23710 non-null  object\n",
      " 2   employment_type     23710 non-null  object\n",
      " 3   job_title           23710 non-null  object\n",
      " 4   salary              23710 non-null  int64 \n",
      " 5   salary_currency     23710 non-null  object\n",
      " 6   salary_in_usd       23710 non-null  int64 \n",
      " 7   employee_residence  23710 non-null  object\n",
      " 8   remote_ratio        23710 non-null  int64 \n",
      " 9   company_location    23710 non-null  object\n",
      " 10  company_size        23710 non-null  object\n",
      "dtypes: int64(4), object(7)\n",
      "memory usage: 2.0+ MB\n",
      "None\n",
      "===========\n",
      "          work_year        salary  salary_in_usd  remote_ratio\n",
      "count  23710.000000  2.371000e+04   23710.000000  23710.000000\n",
      "mean    2023.460565  1.618500e+05  151918.919823     27.954450\n",
      "std        0.693803  2.870241e+05   68370.439498     44.579093\n",
      "min     2020.000000  1.400000e+04   15000.000000      0.000000\n",
      "25%     2023.000000  1.040000e+05  104000.000000      0.000000\n",
      "50%     2024.000000  1.440500e+05  143510.000000      0.000000\n",
      "75%     2024.000000  1.914750e+05  190000.000000    100.000000\n",
      "max     2024.000000  3.040000e+07  800000.000000    100.000000\n",
      "===========\n",
      "371 unique class variables\n",
      "===========\n",
      "(13437, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Description</th>\n",
       "      <th>Scales</th>\n",
       "      <th>Discrete\\Continuous</th>\n",
       "      <th>Unique Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>work_year</td>\n",
       "      <td>Work Year</td>\n",
       "      <td>interval</td>\n",
       "      <td>continuous</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>experience_level</td>\n",
       "      <td>Position Experience Level</td>\n",
       "      <td>ordinal</td>\n",
       "      <td>discrete</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>employment_type</td>\n",
       "      <td>Employment Type</td>\n",
       "      <td>nominal</td>\n",
       "      <td>discrete</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>job_title</td>\n",
       "      <td>Job Title</td>\n",
       "      <td>nominal</td>\n",
       "      <td>discrete</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>salary</td>\n",
       "      <td>Position Salary</td>\n",
       "      <td>ratio</td>\n",
       "      <td>continuous</td>\n",
       "      <td>3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>salary_currency</td>\n",
       "      <td>Currency of Salary</td>\n",
       "      <td>nominal</td>\n",
       "      <td>discrete</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>salary_in_usd</td>\n",
       "      <td>Position Salary (USD)</td>\n",
       "      <td>ratio</td>\n",
       "      <td>continuous</td>\n",
       "      <td>3804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>employee_residence</td>\n",
       "      <td>Employee Residence</td>\n",
       "      <td>nominal</td>\n",
       "      <td>discrete</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>remote_ratio</td>\n",
       "      <td>Percentage of Job Responsibilities Completed R...</td>\n",
       "      <td>ratio</td>\n",
       "      <td>continuous</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>company_location</td>\n",
       "      <td>Company Location</td>\n",
       "      <td>nominal</td>\n",
       "      <td>discrete</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>company_size</td>\n",
       "      <td>Company Size</td>\n",
       "      <td>ordinal</td>\n",
       "      <td>discrete</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Features                                        Description  \\\n",
       "0            work_year                                          Work Year   \n",
       "1     experience_level                          Position Experience Level   \n",
       "2      employment_type                                    Employment Type   \n",
       "3            job_title                                          Job Title   \n",
       "4               salary                                    Position Salary   \n",
       "5      salary_currency                                 Currency of Salary   \n",
       "6        salary_in_usd                              Position Salary (USD)   \n",
       "7   employee_residence                                 Employee Residence   \n",
       "8         remote_ratio  Percentage of Job Responsibilities Completed R...   \n",
       "9     company_location                                   Company Location   \n",
       "10        company_size                                       Company Size   \n",
       "\n",
       "      Scales Discrete\\Continuous  Unique Values  \n",
       "0   interval          continuous              5  \n",
       "1    ordinal            discrete              4  \n",
       "2    nominal            discrete              4  \n",
       "3    nominal            discrete            169  \n",
       "4      ratio          continuous           3400  \n",
       "5    nominal            discrete             24  \n",
       "6      ratio          continuous           3804  \n",
       "7    nominal            discrete             89  \n",
       "8      ratio          continuous              3  \n",
       "9    nominal            discrete             78  \n",
       "10   ordinal            discrete              3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "df = pd.read_csv('salaries.csv') # read in the csv file\n",
    "\n",
    "\n",
    "print(df.info())\n",
    "print('===========')\n",
    "# note that the describe function defaults to using only some variables\n",
    "print(df.describe())\n",
    "print('===========')\n",
    "\n",
    "print(df.select_dtypes(include=['object']).nunique().sum(),\"unique class variables\")\n",
    "print('===========')\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "print(df.shape)\n",
    "\n",
    "# create a data description table\n",
    "data_des = pd.DataFrame()\n",
    "#code adapted from sample Lab 1 Submission\n",
    "data_des['Features'] = df.columns\n",
    "data_des['Description'] = ['Work Year', 'Position Experience Level',\n",
    "                          'Employment Type', 'Job Title',\n",
    "                          'Position Salary', 'Currency of Salary',\n",
    "                          'Position Salary (USD)', 'Employee Residence',\n",
    "                          'Percentage of Job Responsibilities Completed Remotely', 'Company Location', 'Company Size']\n",
    "data_des['Scales'] = ['interval'] + ['ordinal'] + ['nominal']*2 + ['ratio'] + ['nominal']+ ['ratio'] + ['nominal'] + ['ratio'] + ['nominal'] + ['ordinal']\n",
    "data_des['Discrete\\Continuous'] = ['continuous'] + ['discrete']*3 + ['continuous'] + \\\n",
    "                                  ['discrete'] + ['continuous'] + ['discrete'] + ['continuous'] + ['discrete']*2\n",
    "data_des['Unique Values'] = df[df.columns].nunique().values\n",
    "data_des"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d17cddb-8341-4168-af12-5b8806c6c963",
   "metadata": {},
   "source": [
    "### 1.1 Class Variable Definition and Dataset Preparation for Classification/Regression\n",
    "\n",
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created). You have the option of using tf.dataset for processing, but it is not required. \n",
    "\n",
    "#### Class Variables\n",
    "This dataset has 8 features with class variables. Three of the features have n > 20 unique classes, which will be reduced through embedding later in this notebook. 'remote_ratio' is a numeric feature with three values in the dataset indicating the percentage of remote work. The available values are 0, 50, 100. The data source considers these values labeled (0: Non-Remote, Hybrid, & Fully Remote. These will be converted to categorical features in the code below. A  description of all the datatypes is in the code output below. In total, the 371 unique classes exist within categorical features. These will need to be processed for dimensionality reduction and embedding into the model. Additionally, two of the categorical features resemble ordinality, even expressed in the name of the features 'company_**size**' & 'experience_**level**'. They also have low cardinality. These will be converted to labeled variables and they will be crossed with other categorical columns later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585ff5c7-b37a-4dbe-a4a8-76854b0a0556",
   "metadata": {},
   "source": [
    "#### Classification splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6facba8-663b-430c-808f-e4cc94033ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "from sklearn import __version__ as sklearn_version\n",
    "if sklearn_version < '0.18':\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fed431b5-131e-4f3c-b15b-3a4429f6fecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work_year', 'experience_level', 'remote_ratio', 'company_size']\n"
     ]
    }
   ],
   "source": [
    "# Encode ordinal features\n",
    "exp_mapping = {'EN': 0, 'MI': 1, 'SE': 2, 'EX': 3}\n",
    "size_mapping = {'S': 0, 'M': 1, 'L': 2}\n",
    "\n",
    "# Apply the mappings to the entire dataset\n",
    "df['experience_level'] = df['experience_level'].map(exp_mapping)\n",
    "df['company_size'] = df['company_size'].map(size_mapping)\n",
    "\n",
    "# Ensure the numeric headers are float\n",
    "# define variables that should be scaled or made discrete\n",
    "numeric_headers = ['work_year', 'remote_ratio','salary', 'salary_in_usd']\n",
    "df[numeric_headers] = df[numeric_headers].to_numpy().astype(float)\n",
    "\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numerical_cols.remove('salary')\n",
    "numerical_cols.remove('salary_in_usd')\n",
    "print(numerical_cols)\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "label_transformer = LabelEncoder()\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "\n",
    "#FILTER LESS FREQUENT JOBS (FOR TARGET VARIABLE ENGINEERING)\n",
    "# Count occurrences of each job title (this helps with straified splitting)\n",
    "job_title_counts = df['job_title'].value_counts()\n",
    "# Filter out job titles with less than 4 occurrences (Removes 19 jobs, but will calculate a median actually to predict on)\n",
    "# And it prevents freebies to the model! because if it's unseen, as far as the model is concerned, that value is the median.\n",
    "valid_job_titles = job_title_counts[job_title_counts >= 2].index\n",
    "df = df[df['job_title'].isin(valid_job_titles)].copy()\n",
    "\n",
    "\n",
    "# Initial split to separate a test set\n",
    "X = df.drop(columns=['salary', 'salary_in_usd'])\n",
    "y = df['salary_in_usd']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine the full training data for further splits\n",
    "training = deepcopy(pd.concat([X_train, y_train], axis=1))\n",
    "testing = deepcopy(pd.concat([X_test, y_test], axis=1))\n",
    "\n",
    "# Drop any missing values\n",
    "training.dropna(inplace=True)\n",
    "training.reset_index(inplace=True, drop=True)\n",
    "testing.dropna(inplace=True)\n",
    "testing.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Step 2: Calculate the median salary by company size in the training set\n",
    "median_salary_by_job_title = training.groupby('job_title')['salary_in_usd'].median()\n",
    "\n",
    "# Step 3: Create binary target variable based on whether salary is above or below the median for the company size\n",
    "training['salary_binary'] = training.apply(\n",
    "    lambda row: 1 if row['salary_in_usd'] > median_salary_by_job_title[row['job_title']] \n",
    "    else 0, axis=1\n",
    ")\n",
    "\n",
    "# Create binary target variable for the test set\n",
    "testing['salary_binary'] = testing.apply(\n",
    "    lambda row: 1 if row['job_title'] in median_salary_by_job_title\n",
    "    and row['salary_in_usd'] > median_salary_by_job_title[row['job_title']]\n",
    "    else 0, axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Separate features and target for further splitting\n",
    "X_train = training.drop(columns=[ 'salary_in_usd', 'salary_binary'])\n",
    "y_train = training['salary_binary']\n",
    "X_test = testing.drop(columns=[ 'salary_in_usd', 'salary_binary'])\n",
    "y_test= testing['salary_binary']\n",
    "\n",
    "#Scale testing data\n",
    "# Preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test = preprocessing_pipeline.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e22899-af88-4e8c-ae4f-c4b76dca4537",
   "metadata": {},
   "source": [
    "#### Pre-Processing with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5480a092-2e10-479c-86f1-71c07bb1e748",
   "metadata": {},
   "source": [
    "### 1.2 Crossed Features\n",
    "#### 1 Employee Residence x Remote Ratio\n",
    "- How is the salary in USD influenced by the employee's type of salary paid? This is probably a strong indicator of pay based on area. If the pay is not in USD, it might be more of a comtract role\n",
    "- Total = 9 X 4 = 36\n",
    "#### 2 Company Location x Company Size\n",
    "- Companies in the US will probably be larger as it is the leading big tech country.\n",
    "- Total = 9 * 3 = 27\n",
    "#### 3 Job Title x Experience Level\n",
    "- Specifics about each position (such as \"Data Science Lead\") suggest the experience/authority level\n",
    "- Total = 13 * 4 = 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d907cef-39f4-45b2-bed9-7319964d6555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected cross-product features: [('job_title', 'location')]\n",
      "Selected cross-product features: ['job_title', 'location']\n",
      "Mean Absolute Error: 0.6099600889264719\n",
      "Root Mean Squared Error: 0.7282836328108407\n",
      "Root Mean Squared Error: 0.663717982438963\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Identify groups of features for cross-product\n",
    "cross_product_features = [('job_title', 'location')]\n",
    "print(f\"Selected cross-product features: {cross_product_features}\")\n",
    "# Identify groups of features for cross-product\n",
    "cross_product_features = ['job_title', 'location']\n",
    "print(f\"Selected cross-product features: {cross_product_features}\")\n",
    "\n",
    "# Metrics for evaluation\n",
    "precision_scorer = make_scorer(precision_score)\n",
    "recall_scorer = make_scorer(recall_score)\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Choose the cross-validation method\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# Create and evaluate the model using cross-validation\n",
    "model = LogisticRegression()\n",
    "precision_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=precision_scorer)\n",
    "recall_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=recall_scorer)\n",
    "f1_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=f1_scorer)\n",
    "\n",
    "print(f\"Mean Absolute Error: {precision_scores.mean()}\")\n",
    "print(f\"Root Mean Squared Error: {recall_scores.mean()}\")\n",
    "print(f\"Root Mean Squared Error: {f1_scores.mean()}\")\n",
    "# Use StratifiedShuffleSplit to ensure balanced class distribution\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f406d039-5cac-4ab0-a9bf-2273ca3acefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import FeatureSpace\n",
    "\n",
    "# Example One: Just lump everything together, and concatenate\n",
    "feature_space = FeatureSpace(\n",
    "    features={\n",
    "        # Categorical feature encoded as string\n",
    "        # \"experience_level\": FeatureSpace.string_categorical(num_oov_indices=0),\n",
    "        \"employment_type\": FeatureSpace.string_categorical(num_oov_indices=0,),\n",
    "        \"job_title\": FeatureSpace.string_categorical(num_oov_indices=1),\n",
    "        \"salary_currency\": FeatureSpace.string_categorical(num_oov_indices=1),\n",
    "        \"employee_residence\": FeatureSpace.string_categorical(num_oov_indices=1),\n",
    "        \"company_location\": FeatureSpace.string_categorical(num_oov_indices=1),\n",
    "\n",
    "        # Categorical feature encoded as integers\n",
    "        \"remote_ratio\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"company_size\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"experience_level\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "\n",
    "        # Numerical features to normalize (normalization will be learned)\n",
    "        # learns the mean, variance, and if to invert (3 parameters)\n",
    "        # \"salary_in_usd\": FeatureSpace.float_normalized(),\n",
    "\n",
    "        \"work_year\": FeatureSpace.float_normalized()\n",
    "            },\n",
    "    output_mode=\"concat\", # can also be a dict, processed internally\n",
    ")\n",
    "\n",
    "# now that we have specified the preprocessing, let's run it on the data\n",
    "ds_train = create_dataset_from_dataframe(training)\n",
    "ds_test = create_dataset_from_dataframe(testing)\n",
    "# # create a version of the dataset that can be iterated without labels\n",
    "# train_ds_with_no_labels = ds_train.map(lambda x, _: x)\n",
    "# feature_space.adapt(train_ds_with_no_labels) # inititalize the feature map to this data\n",
    "\n",
    "# # the adapt function allows the model to learn one-hot encoding sizes\n",
    "# # now define a preprocessing operation that returns the processed features\n",
    "# preproc_ds_train = ds_train.map(lambda x, y: (feature_space(x), y),\n",
    "#                                      num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# # run it so that we can use the pre-processed data\n",
    "# preproc_ds_train = preproc_ds_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# # do the same for the test set\n",
    "# preproc_ds_test = ds_test.map(lambda x, y: (feature_space(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# preproc_ds_test = preproc_ds_test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "218abdbe-30d2-48c4-9676-0f38b8f08358",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`adapt()` can only be called on a tf.data.Dataset. Received instead:   (0, 0)\t-0.5716510631127374\n  (0, 1)\t-2.1068229452963636\n  (0, 2)\t1.470223778295884\n  (0, 3)\t3.8278945817592067\n  (0, 6)\t1.0\n  (0, 62)\t1.0\n  (0, 163)\t1.0\n  (0, 204)\t1.0\n  (0, 285)\t1.0\n  (1, 0)\t-0.5716510631127374\n  (1, 1)\t-0.7332419507135669\n  (1, 2)\t1.470223778295884\n  (1, 3)\t-0.14087007009081992\n  (1, 6)\t1.0\n  (1, 125)\t1.0\n  (1, 164)\t1.0\n  (1, 207)\t1.0\n  (1, 288)\t1.0\n  (2, 0)\t0.7573726560667198\n  (2, 1)\t0.64033904386923\n  (2, 2)\t-0.6948718502166717\n  (2, 3)\t-0.14087007009081992\n  (2, 6)\t1.0\n  (2, 62)\t1.0\n  (2, 175)\t1.0\n  :\t:\n  (10731, 2)\t-0.6948718502166717\n  (10731, 3)\t-0.14087007009081992\n  (10731, 6)\t1.0\n  (10731, 125)\t1.0\n  (10731, 175)\t1.0\n  (10731, 258)\t1.0\n  (10731, 335)\t1.0\n  (10732, 0)\t0.7573726560667198\n  (10732, 1)\t0.64033904386923\n  (10732, 2)\t-0.6948718502166717\n  (10732, 3)\t-0.14087007009081992\n  (10732, 6)\t1.0\n  (10732, 154)\t1.0\n  (10732, 175)\t1.0\n  (10732, 258)\t1.0\n  (10732, 335)\t1.0\n  (10733, 0)\t0.7573726560667198\n  (10733, 1)\t0.64033904386923\n  (10733, 2)\t-0.6948718502166717\n  (10733, 3)\t-0.14087007009081992\n  (10733, 6)\t1.0\n  (10733, 125)\t1.0\n  (10733, 175)\t1.0\n  (10733, 199)\t1.0\n  (10733, 280)\t1.0 (of type <class 'scipy.sparse._csr.csr_matrix'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfeature_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs7324_env\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\feature_space.py:499\u001b[0m, in \u001b[0;36mFeatureSpace.adapt\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madapt\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[1;32m--> 499\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    500\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`adapt()` can only be called on a tf.data.Dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    501\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived instead: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m         )\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_adaptable_preprocessors():\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;66;03m# Call adapt() on each individual adaptable layer.\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[38;5;66;03m# and call the layer's `_adapt_function` on each batch\u001b[39;00m\n\u001b[0;32m    510\u001b[0m         \u001b[38;5;66;03m# to simulate the behavior of adapt() in a more performant fashion.\u001b[39;00m\n\u001b[0;32m    512\u001b[0m         feature_dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[name])\n",
      "\u001b[1;31mValueError\u001b[0m: `adapt()` can only be called on a tf.data.Dataset. Received instead:   (0, 0)\t-0.5716510631127374\n  (0, 1)\t-2.1068229452963636\n  (0, 2)\t1.470223778295884\n  (0, 3)\t3.8278945817592067\n  (0, 6)\t1.0\n  (0, 62)\t1.0\n  (0, 163)\t1.0\n  (0, 204)\t1.0\n  (0, 285)\t1.0\n  (1, 0)\t-0.5716510631127374\n  (1, 1)\t-0.7332419507135669\n  (1, 2)\t1.470223778295884\n  (1, 3)\t-0.14087007009081992\n  (1, 6)\t1.0\n  (1, 125)\t1.0\n  (1, 164)\t1.0\n  (1, 207)\t1.0\n  (1, 288)\t1.0\n  (2, 0)\t0.7573726560667198\n  (2, 1)\t0.64033904386923\n  (2, 2)\t-0.6948718502166717\n  (2, 3)\t-0.14087007009081992\n  (2, 6)\t1.0\n  (2, 62)\t1.0\n  (2, 175)\t1.0\n  :\t:\n  (10731, 2)\t-0.6948718502166717\n  (10731, 3)\t-0.14087007009081992\n  (10731, 6)\t1.0\n  (10731, 125)\t1.0\n  (10731, 175)\t1.0\n  (10731, 258)\t1.0\n  (10731, 335)\t1.0\n  (10732, 0)\t0.7573726560667198\n  (10732, 1)\t0.64033904386923\n  (10732, 2)\t-0.6948718502166717\n  (10732, 3)\t-0.14087007009081992\n  (10732, 6)\t1.0\n  (10732, 154)\t1.0\n  (10732, 175)\t1.0\n  (10732, 258)\t1.0\n  (10732, 335)\t1.0\n  (10733, 0)\t0.7573726560667198\n  (10733, 1)\t0.64033904386923\n  (10733, 2)\t-0.6948718502166717\n  (10733, 3)\t-0.14087007009081992\n  (10733, 6)\t1.0\n  (10733, 125)\t1.0\n  (10733, 175)\t1.0\n  (10733, 199)\t1.0\n  (10733, 280)\t1.0 (of type <class 'scipy.sparse._csr.csr_matrix'>)"
     ]
    }
   ],
   "source": [
    "feature_space.adapt(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f73b89-572e-4c38-892c-871f56735cb8",
   "metadata": {
    "id": "w8ACHxrvZmTU"
   },
   "source": [
    "## 2. Modeling: Define the Wide and Deep Network Models\n",
    "\n",
    "We will use Keras to develop three alternative broad and deep networks to categorize wage data. Each network will have a unique design to investigate how different combinations of broad and deep components impact the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114641eb-ef86-4ae5-9ab0-57f6fd085c99",
   "metadata": {
    "id": "VU1vfOq0Zp6S"
   },
   "source": [
    "### Model 1: Basic Wide and Deep Network\n",
    "\n",
    "This model has a wide component (a basic linear layer) and a deep component (three hidden layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ba6a534-3ce4-43dc-9020-d7229969b51f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`adapt()` can only be called on a tf.data.Dataset. Received instead:   (0, 0)\t-0.5716510631127374\n  (0, 1)\t-2.1068229452963636\n  (0, 2)\t1.470223778295884\n  (0, 3)\t3.8278945817592067\n  (0, 6)\t1.0\n  (0, 62)\t1.0\n  (0, 163)\t1.0\n  (0, 204)\t1.0\n  (0, 285)\t1.0\n  (1, 0)\t-0.5716510631127374\n  (1, 1)\t-0.7332419507135669\n  (1, 2)\t1.470223778295884\n  (1, 3)\t-0.14087007009081992\n  (1, 6)\t1.0\n  (1, 125)\t1.0\n  (1, 164)\t1.0\n  (1, 207)\t1.0\n  (1, 288)\t1.0\n  (2, 0)\t0.7573726560667198\n  (2, 1)\t0.64033904386923\n  (2, 2)\t-0.6948718502166717\n  (2, 3)\t-0.14087007009081992\n  (2, 6)\t1.0\n  (2, 62)\t1.0\n  (2, 175)\t1.0\n  :\t:\n  (10731, 2)\t-0.6948718502166717\n  (10731, 3)\t-0.14087007009081992\n  (10731, 6)\t1.0\n  (10731, 125)\t1.0\n  (10731, 175)\t1.0\n  (10731, 258)\t1.0\n  (10731, 335)\t1.0\n  (10732, 0)\t0.7573726560667198\n  (10732, 1)\t0.64033904386923\n  (10732, 2)\t-0.6948718502166717\n  (10732, 3)\t-0.14087007009081992\n  (10732, 6)\t1.0\n  (10732, 154)\t1.0\n  (10732, 175)\t1.0\n  (10732, 258)\t1.0\n  (10732, 335)\t1.0\n  (10733, 0)\t0.7573726560667198\n  (10733, 1)\t0.64033904386923\n  (10733, 2)\t-0.6948718502166717\n  (10733, 3)\t-0.14087007009081992\n  (10733, 6)\t1.0\n  (10733, 125)\t1.0\n  (10733, 175)\t1.0\n  (10733, 199)\t1.0\n  (10733, 280)\t1.0 (of type <class 'scipy.sparse._csr.csr_matrix'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!pip install pydot\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#!pip install graphviz\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mfeature_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# these are the placeholder inputs in the computation graph BEFORE\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# applying and transformations\u001b[39;00m\n\u001b[0;32m      6\u001b[0m dict_inputs \u001b[38;5;241m=\u001b[39m feature_space\u001b[38;5;241m.\u001b[39mget_inputs()  \u001b[38;5;66;03m#getting inputs is WAY easier now\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs7324_env\\lib\\site-packages\\keras\\src\\layers\\preprocessing\\feature_space.py:499\u001b[0m, in \u001b[0;36mFeatureSpace.adapt\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madapt\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[1;32m--> 499\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    500\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`adapt()` can only be called on a tf.data.Dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    501\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived instead: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m         )\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_adaptable_preprocessors():\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;66;03m# Call adapt() on each individual adaptable layer.\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[38;5;66;03m# and call the layer's `_adapt_function` on each batch\u001b[39;00m\n\u001b[0;32m    510\u001b[0m         \u001b[38;5;66;03m# to simulate the behavior of adapt() in a more performant fashion.\u001b[39;00m\n\u001b[0;32m    512\u001b[0m         feature_dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[name])\n",
      "\u001b[1;31mValueError\u001b[0m: `adapt()` can only be called on a tf.data.Dataset. Received instead:   (0, 0)\t-0.5716510631127374\n  (0, 1)\t-2.1068229452963636\n  (0, 2)\t1.470223778295884\n  (0, 3)\t3.8278945817592067\n  (0, 6)\t1.0\n  (0, 62)\t1.0\n  (0, 163)\t1.0\n  (0, 204)\t1.0\n  (0, 285)\t1.0\n  (1, 0)\t-0.5716510631127374\n  (1, 1)\t-0.7332419507135669\n  (1, 2)\t1.470223778295884\n  (1, 3)\t-0.14087007009081992\n  (1, 6)\t1.0\n  (1, 125)\t1.0\n  (1, 164)\t1.0\n  (1, 207)\t1.0\n  (1, 288)\t1.0\n  (2, 0)\t0.7573726560667198\n  (2, 1)\t0.64033904386923\n  (2, 2)\t-0.6948718502166717\n  (2, 3)\t-0.14087007009081992\n  (2, 6)\t1.0\n  (2, 62)\t1.0\n  (2, 175)\t1.0\n  :\t:\n  (10731, 2)\t-0.6948718502166717\n  (10731, 3)\t-0.14087007009081992\n  (10731, 6)\t1.0\n  (10731, 125)\t1.0\n  (10731, 175)\t1.0\n  (10731, 258)\t1.0\n  (10731, 335)\t1.0\n  (10732, 0)\t0.7573726560667198\n  (10732, 1)\t0.64033904386923\n  (10732, 2)\t-0.6948718502166717\n  (10732, 3)\t-0.14087007009081992\n  (10732, 6)\t1.0\n  (10732, 154)\t1.0\n  (10732, 175)\t1.0\n  (10732, 258)\t1.0\n  (10732, 335)\t1.0\n  (10733, 0)\t0.7573726560667198\n  (10733, 1)\t0.64033904386923\n  (10733, 2)\t-0.6948718502166717\n  (10733, 3)\t-0.14087007009081992\n  (10733, 6)\t1.0\n  (10733, 125)\t1.0\n  (10733, 175)\t1.0\n  (10733, 199)\t1.0\n  (10733, 280)\t1.0 (of type <class 'scipy.sparse._csr.csr_matrix'>)"
     ]
    }
   ],
   "source": [
    "#!pip install pydot\n",
    "#!pip install graphviz\n",
    "feature_space.adapt(X_train)\n",
    "# these are the placeholder inputs in the computation graph BEFORE\n",
    "# applying and transformations\n",
    "dict_inputs = feature_space.get_inputs()  #getting inputs is WAY easier now\n",
    "\n",
    "# these are the encoded features after they have been processed\n",
    "# We can use these as additional inpits into the computation graph\n",
    "encoded_features = feature_space.get_encoded_features() # these features have been encoded\n",
    "\n",
    "    \n",
    "# using feature space above, this will result in 131 concatenated features\n",
    "# this is calucalted based on the one-hot encodings for each category\n",
    "# now lets create some layers with Keras\n",
    "x = keras.layers.Dense(128, activation=\"relu\")(encoded_features)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "predictions = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# we can now create two input/outputs to the computation graph\n",
    "\n",
    "# this expects features already transformed\n",
    "training_model = keras.Model(inputs=encoded_features,\n",
    "                             outputs=predictions)\n",
    "training_model.compile(\n",
    "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# this expects features that are not transformed\n",
    "inference_model = keras.Model(inputs=dict_inputs,\n",
    "                              outputs=predictions)\n",
    "inference_model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "inference_model.summary()\n",
    "\n",
    "# plot_model(\n",
    "#     training_model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "#     rankdir='LR', expand_nested=False, dpi=96\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7cf43-31a5-4ffe-9137-30af3e01a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import FeatureSpace\n",
    "\n",
    "# Crossing columns together \n",
    "feature_space1 = FeatureSpace(\n",
    "    features={\n",
    "        # Categorical feature encoded as string\n",
    "        # \"experience_level\": FeatureSpace.string_categorical(num_oov_indices=0),\n",
    "        \"employment_type\": FeatureSpace.string_categorical(num_oov_indices=0,),\n",
    "        \"job_title\": FeatureSpace.string_categorical(num_oov_indices=0),\n",
    "        \"salary_currency\": FeatureSpace.string_categorical(num_oov_indices=0),\n",
    "        \"employee_residence\": FeatureSpace.string_categorical(num_oov_indices=0),\n",
    "        \"company_location\": FeatureSpace.string_categorical(num_oov_indices=0),\n",
    "\n",
    "        # Categorical feature encoded as integers\n",
    "        \"remote_ratio\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"company_size\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"experience_level\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \n",
    "        \n",
    "        # Numerical features to normalize (normalization will be learned)\n",
    "        # learns the mean, variance, and if to invert (3 parameters)\n",
    "        # \"salary_in_usd\": FeatureSpace.float_normalized(),\n",
    "        \"work_year\": FeatureSpace.float_normalized(),\n",
    "        \n",
    "            },\n",
    "    # Specify feature cross with a custom crossing dim\n",
    "    crosses=[\n",
    "        FeatureSpace.cross(\n",
    "            feature_names=('employee_residence','employment_type'), # dims: 9 x 3 x 4 = 108 \n",
    "            crossing_dim=9*4),\n",
    "        FeatureSpace.cross(\n",
    "            feature_names=('company_location','company_size'), # 8 x 3 = 24\n",
    "            crossing_dim=8*3),\n",
    "        FeatureSpace.cross(\n",
    "            feature_names=('job_title','experience_level'), # 12 x 4 = 48\n",
    "            crossing_dim=12*4),\n",
    "    ],\n",
    "    output_mode=\"concat\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b286d48-83ec-4c50-b132-7463566f8426",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction (Square Root Heuristic)\n",
    "\n",
    "In TensorFlow, embeddings are used to convert categorical data into continuous vector spaces, where similar categories are mapped closer together. The embedding layer in TensorFlow is a trainable layer that learns a fixed-size continuous vector representation for each category. This is especially useful for high cardinality categorical features. By choosing to embed certain classes with high cardinality, we reduced the dimensionality of the dataset. This was particularly useful in Keras, as the only real requirement is to encode the data one-hot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8259998a-0fc3-481b-bc8a-bcf3bfb0c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Flatten\n",
    "\n",
    "def setup_embedding_from_categorical(feature_space, col_name):\n",
    "    # what the maximum integer value for this variable?\n",
    "    # which is the same as the number of categories\n",
    "    N = len(feature_space.preprocessors[col_name].get_vocabulary())\n",
    "    \n",
    "    # get the output from the feature space, which is input to embedding\n",
    "    x = feature_space.preprocessors[col_name].output\n",
    "    \n",
    "    # now use an embedding to deal with integers from feature space\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col_name+'_embed')(x)\n",
    "    \n",
    "    x = Flatten()(x) # get rid of that pesky extra dimension (for time of embedding)\n",
    "    \n",
    "    return x # return the tensor here \n",
    "\n",
    "# # add explanation of this pre-processing here\n",
    "# train_ds_with_no_labels = ds_train.map(lambda x, _: x)\n",
    "# feature_space.adapt(train_ds_with_no_labels)\n",
    "\n",
    "def setup_embedding_from_crossing(feature_space, col_name):\n",
    "    # what the maximum integer value for this variable?\n",
    "    \n",
    "    # get the size of the feature\n",
    "    N = feature_space.crossers[col_name].num_bins\n",
    "    x = feature_space.crossers[col_name].output\n",
    "    \n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col_name+'_embed')(x)\n",
    "    \n",
    "    x = Flatten()(x) # get rid of that pesky extra dimension (for time of embedding)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986c680b-4b76-448d-b45f-55785a36b040",
   "metadata": {},
   "source": [
    "From the raw data, the model has almost 25,000 parameters! That is a lot of parameters and could lead to longer computation times. To reduce dimensionality, we then passed into the TensorFlow model, and the gradient of the distances/connections of these feature-specific classes is solved, returning an output of a specified reduced size ($\\sqrt{n}$) that still maintains the majority of the information from the original data. These approximations are the output of the feature space in the below code.\n",
    "\n",
    "##### *'salary_currenecy'*\n",
    "- - **Number of dimensions after reduction = $\\sqrt{24} \\approx 4$**\n",
    "##### *'company_location'*\n",
    "- - **Number of dimensions after reduction = $\\sqrt{78} \\approx 8$**\n",
    "##### *'employee_residence'*\n",
    "- **Number of dimensions after reduction = $\\sqrt{89} \\approx 9$**\n",
    "##### *'job_title'*\n",
    "- **Number of dimensions after reduction = $\\sqrt{169} \\approx 12$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c14ac-6d84-4409-bd34-d573a1c19e91",
   "metadata": {},
   "source": [
    "### 2.1 Running the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "471a94e0-730b-4f64-8a7f-343de28b17c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the loss function we plan to use\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "# import a built in optimizer\n",
    "# I am using legacy for an M1 chipset speed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "\n",
    "\n",
    "def create_model1(feature_space):\n",
    "    \n",
    "    dict_inputs = feature_space.get_inputs() # need to use unprocessed features here, to gain access to each output\n",
    "    encoded_features = feature_space.get_encoded_features()\n",
    "    print(\"dict_inputs:\",dict_inputs, \"\\n\", \"encoded_features:\",encoded_features, \"\\n\")\n",
    "    # we need to create separate lists for each branch\n",
    "    crossed_outputs = []\n",
    "    \n",
    "    # for each crossed variable, make an embedding\n",
    "    for col in feature_space.crossers.keys():\n",
    "        x = setup_embedding_from_crossing(feature_space, col)\n",
    "    \n",
    "        # save these outputs in list to concatenate later\n",
    "        crossed_outputs.append(x)\n",
    "        \n",
    "    print(\"xed outputs\",crossed_outputs)\n",
    "    # now concatenate the outputs and add a fully connected layer\n",
    "    wide_branch = Concatenate(name='wide_concat')(crossed_outputs)\n",
    "    \n",
    "    # reset this input branch\n",
    "    all_deep_branch_outputs = []\n",
    "    \n",
    "    # for each numeric variable, just add it in after embedding\n",
    "    for idx,col in enumerate(numeric_headers):\n",
    "        x = feature_space.preprocessors[col].output\n",
    "        # x = tf.cast(132,float) # cast an integer as a float here\n",
    "        all_deep_branch_outputs.append(x)\n",
    "    \n",
    "    # for each categorical variable\n",
    "    for col in categorical_headers:\n",
    "    \n",
    "        # get the output tensor from ebedding layer\n",
    "        x = setup_embedding_from_categorical(feature_space, col)\n",
    "        # save these outputs in list to concatenate later\n",
    "        all_deep_branch_outputs.append(x)\n",
    "    \n",
    "    print(len(all_deep_branch_outputs))\n",
    "    # Define deep branch with numeric and categorical features\n",
    "    print(len(dict_inputs.keys()))\n",
    "    print(encoded_features)\n",
    "    \n",
    "\n",
    "    deep_branch = Concatenate(name='embed_concat')(all_deep_branch_outputs)\n",
    "    deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "    deep_branch = Dense(units=25,activation='relu', name='deep2')(deep_branch)\n",
    "    deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "    \n",
    "    # merge the deep and wide branch\n",
    "    final_branch = Concatenate(name='concat_deep_wide')([deep_branch, wide_branch])\n",
    "    final_branch = Dense(units=1,activation='sigmoid',\n",
    "                         name='combined')(final_branch)\n",
    "    print(final_branch)\n",
    "    training_model = keras.Model(inputs=dict_inputs, outputs=final_branch)\n",
    "    training_model.compile(\n",
    "        optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    print(\"error\")\n",
    "    model = tf.keras.Model(inputs=dict_inputs, outputs=final_branch)\n",
    "    print(\"error\")\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print(\"error\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "e7fef176-fe7f-442b-85c3-1d70faceeb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Input\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# def create_model1(feature_space):\n",
    "#     dict_inputs = feature_space.get_inputs()  # Get the inputs correctly\n",
    "#     encoded_features = feature_space.get_encoded_features()\n",
    "    \n",
    "#     # Define the model architecture\n",
    "#     x = layers.Dense(128, activation='relu')(encoded_features)\n",
    "#     x =layers.Dense(128, activation='relu')(x)\n",
    "#     output = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "#     model = Model(inputs=dict_inputs, outputs=output)\n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0280f8d4-e03b-4c19-a12d-fb885af13117",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m [] \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, val_index \u001b[38;5;129;01min\u001b[39;00m sss\u001b[38;5;241m.\u001b[39msplit(X_train, y_train):\n\u001b[0;32m      3\u001b[0m     X_train_split, X_val_split \u001b[38;5;241m=\u001b[39m X_train[train_index], X_train[val_index]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "[] = results\n",
    "for train_index, val_index in sss.split(X_train, y_train):\n",
    "    X_train_split, X_val_split = X_train[train_index], X_train[val_index]\n",
    "    y_train_split, y_val_split = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    # Encode and scale the splits\n",
    "    X_train_split, X_val_split = encode_and_scale(X_train_split, X_val_split)\n",
    "\n",
    "    # Combine features and target for processing\n",
    "    train_data = pd.concat([X_train_split, y_train_split], axis=1)\n",
    "    val_data = pd.concat([X_val_split, y_val_split], axis=1)\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    ds_train = create_dataset_from_dataframe(train_data,'salary_binary')\n",
    "    ds_val = create_dataset_from_dataframe(val_data,'salary_binary')\n",
    "    \n",
    "    # Adapt the feature space to the training data\n",
    "    feature_space1.adapt(ds_train.map(lambda x, _: x))\n",
    "    \n",
    "    # Define preprocessing operation\n",
    "    preproc_ds_train = ds_train.map(lambda x, y: (feature_space(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    preproc_ds_val = ds_val.map(lambda x, y: (feature_space(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "  \n",
    "    # Train the model with the preprocessed datasets\n",
    "    # model1 = create_model1(feature_space1)\n",
    "    model1 = create_model1(feature_space1)\n",
    "    history = model1.fit(preproc_ds_train, validation_data=preproc_ds_val, epochs=10)\n",
    "    results.append(history)\n",
    "\n",
    "# Evaluate the model on the test set (only once, not in the loop)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "ds_test = create_dataset_from_dataframe(test_data)\n",
    "preproc_ds_test = ds_test.map(lambda x, y: (feature_space(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "test_loss, test_accuracy = create_model.evaluate(preproc_ds_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "9549fe39-9db0-4b5f-9a18-d8f9ebc259d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model1(feature_space):\n",
    "    dict_inputs = feature_space.get_inputs()\n",
    "    encoded_features = feature_space.get_encoded_features()\n",
    "\n",
    "    crossed_outputs = []\n",
    "    for col in feature_space.crossers.keys():\n",
    "        x = setup_embedding_from_crossing(feature_space, col)\n",
    "        crossed_outputs.append(x)\n",
    "\n",
    "    wide_branch = Concatenate(name='wide_concat')(crossed_outputs)\n",
    "\n",
    "    all_deep_branch_outputs = []\n",
    "    numeric_headers = ['salary_in_usd', 'work_year']  # Example numeric headers, adjust as necessary\n",
    "    categorical_headers = ['employment_type', 'job_title', 'salary_currency', 'employee_residence', 'company_location']  # Example categorical headers, adjust as necessary\n",
    "\n",
    "    for idx, col in enumerate(numeric_headers):\n",
    "        x = feature_space.preprocessors[col].output\n",
    "        all_deep_branch_outputs.append(x)\n",
    "\n",
    "    for col in categorical_headers:\n",
    "        x = setup_embedding_from_categorical(feature_space, col)\n",
    "        all_deep_branch_outputs.append(x)\n",
    "\n",
    "    deep_branch = Concatenate(name='embed_concat')(all_deep_branch_outputs)\n",
    "    deep_branch = Dense(units=50, activation='relu', name='deep1')(deep_branch)\n",
    "    deep_branch = Dense(units=25, activation='relu', name='deep2')(deep_branch)\n",
    "    deep_branch = Dense(units=10, activation='relu', name='deep3')(deep_branch)\n",
    "\n",
    "    final_branch = Concatenate(name='concat_deep_wide')([deep_branch, wide_branch])\n",
    "    final_branch = Dense(units=1, activation='sigmoid', name='combined')(final_branch)\n",
    "\n",
    "    model = Model(inputs=dict_inputs, outputs=final_branch)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "dbc9cf18-f21d-4e19-af21-e87bf908143a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs7324_env\\lib\\site-packages\\pandas\\core\\indexing.py:1714\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1713\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1716\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs7324_env\\lib\\site-packages\\pandas\\core\\generic.py:4150\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   4141\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4142\u001b[0m \u001b[38;5;124;03mInternal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   4143\u001b[0m \u001b[38;5;124;03mattribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4148\u001b[0m \u001b[38;5;124;03mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   4149\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4150\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4151\u001b[0m \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs7324_env\\lib\\site-packages\\pandas\\core\\generic.py:4130\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4126\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4127\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4128\u001b[0m     )\n\u001b[1;32m-> 4130\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4132\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4134\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4136\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4137\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs7324_env\\lib\\site-packages\\pandas\\core\\internals\\managers.py:891\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    890\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[axis]\n\u001b[1;32m--> 891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_convert_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs7324_env\\lib\\site-packages\\pandas\\core\\indexers\\utils.py:282\u001b[0m, in \u001b[0;36mmaybe_convert_indices\u001b[1;34m(indices, n, verify)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n",
      "\u001b[1;31mIndexError\u001b[0m: indices are out-of-bounds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[448], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, val_index \u001b[38;5;129;01min\u001b[39;00m sss\u001b[38;5;241m.\u001b[39msplit(X_train, y_train):\n\u001b[0;32m      3\u001b[0m     X_train_split, X_val_split \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39miloc[train_index], X_train\u001b[38;5;241m.\u001b[39miloc[val_index]\n\u001b[1;32m----> 4\u001b[0m     y_train_split, y_val_split \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39miloc[train_index], \u001b[43my_val_split\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Encode and scale the splits\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     X_train_split, X_val_split \u001b[38;5;241m=\u001b[39m encode_and_scale(X_train_split, X_val_split)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs7324_env\\lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs7324_env\\lib\\site-packages\\pandas\\core\\indexing.py:1743\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1741\u001b[0m \u001b[38;5;66;03m# a list of integers\u001b[39;00m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_list_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# a single integer\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1747\u001b[0m     key \u001b[38;5;241m=\u001b[39m item_from_zerodim(key)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs7324_env\\lib\\site-packages\\pandas\\core\\indexing.py:1717\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1716\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[39;00m\n\u001b[1;32m-> 1717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional indexers are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for train_index, val_index in sss.split(X_train, y_train):\n",
    "    X_train_split, X_val_split = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_split, y_val_split = y_train.iloc[train_index], y_val_split.iloc[val_index]\n",
    "\n",
    "    # Encode and scale the splits\n",
    "    X_train_split, X_val_split = encode_and_scale(X_train_split, X_val_split)\n",
    "\n",
    "    # Combine features and target for processing\n",
    "    train_data = pd.concat([X_train_split, y_train_split], axis=1)\n",
    "    val_data = pd.concat([X_val_split, y_val_split], axis=1)\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    ds_train = create_dataset_from_dataframe(train_data, 'salary_binary')\n",
    "    ds_val = create_dataset_from_dataframe(val_data, 'salary_binary')\n",
    "    \n",
    "    # Adapt the feature space to the training data\n",
    "    feature_space1.adapt(ds_train.map(lambda x, _: x))\n",
    "    \n",
    "    # Define preprocessing operation\n",
    "    preproc_ds_train = ds_train.map(lambda x, y: (feature_space1(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    preproc_ds_val = ds_val.map(lambda x, y: (feature_space1(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "  \n",
    "    # Train the model with the preprocessed datasets\n",
    "    model1 = create_model1(feature_space1)\n",
    "    history = model1.fit(preproc_ds_train, validation_data=preproc_ds_val, epochs=10)\n",
    "    results.append(history)\n",
    "\n",
    "# Evaluate the model on the test set (only once, not in the loop)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "ds_test = create_dataset_from_dataframe(test_data)\n",
    "preproc_ds_test = ds_test.map(lambda x, y: (feature_space1(x), y), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "test_loss, test_accuracy = model1.evaluate(preproc_ds_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f36fa-2e59-4935-b45e-107133bd8285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydot\n",
    "#!pip install graphviz\n",
    "# these are the placeholder inputs in the computation graph BEFORE \n",
    "# applying and transformations\n",
    "dict_inputs = feature_space.get_inputs()  #getting inputs is WAY easier now\n",
    "\n",
    "# these are the encoded features after they have been processed\n",
    "# We can use these as additional inpits into the computation graph\n",
    "encoded_features = feature_space.get_encoded_features() # these features have been encoded\n",
    "\n",
    "# using feature space above, this will result in 131 concatenated features\n",
    "# this is calucalted based on the one-hot encodings for each category\n",
    "\n",
    "# now lets create some layers with Keras\n",
    "x = keras.layers.Dense(64, activation=\"relu\")(encoded_features)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "predictions = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# we can now create two input/outputs to the computation graph\n",
    "\n",
    "# this expects features already transformed\n",
    "training_model = keras.Model(inputs=encoded_features, \n",
    "                             outputs=predictions)\n",
    "training_model.compile(\n",
    "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# this expects features that are not transformed \n",
    "inference_model = keras.Model(inputs=dict_inputs, \n",
    "                              outputs=predictions)\n",
    "inference_model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "inference_model.summary()\n",
    "\n",
    "# plot_model(\n",
    "#     training_model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "#     rankdir='LR', expand_nested=False, dpi=96\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f52e11-c6e6-41fa-b378-06e053bb32b3",
   "metadata": {},
   "source": [
    "### 1.2 Crossed Features\n",
    "#### 1 Employee Residence x Remote Ratio\n",
    "- How is the salary in USD influenced by the employee's type of salary paid? This is probably a strong indicator of pay based on area. If the pay is not in USD, it might be more of a comtract role\n",
    "- Total = 9 X 4 = 36\n",
    "#### 2 Company Location x Company Size\n",
    "- Companies in the US will probably be larger as it is the leading big tech country.\n",
    "- Total = 9 * 3 = 27\n",
    "#### 3 Job Title x Experience Level\n",
    "- Specifics about each position (such as \"Data Science Lead\") suggest the experience/authority level\n",
    "- Total = 13 * 4 = 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de39213c-b3bc-4cd3-a931-6d743ba2adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import FeatureSpace\n",
    "\n",
    "# Crossing columns together \n",
    "feature_space = FeatureSpace(\n",
    "    features={\n",
    "        # Categorical feature encoded as string\n",
    "        # \"experience_level\": FeatureSpace.string_categorical(num_oov_indices=0),\n",
    "        \"employment_type\": FeatureSpace.string_categorical(num_oov_indices=0,),\n",
    "        \"job_title\": FeatureSpace.string_categorical(num_oov_indices=0),\n",
    "        \"salary_currency\": FeatureSpace.string_categorical(num_oov_indices=0),\n",
    "        \"employee_residence\": FeatureSpace.string_categorical(num_oov_indices=0),\n",
    "        \"company_location\": FeatureSpace.string_categorical(num_oov_indices=0),\n",
    "\n",
    "        # Categorical feature encoded as integers\n",
    "        \"remote_ratio\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"company_size\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \"experience_level\": FeatureSpace.integer_categorical(num_oov_indices=0),\n",
    "        \n",
    "        \n",
    "        # Numerical features to normalize (normalization will be learned)\n",
    "        # learns the mean, variance, and if to invert (3 parameters)\n",
    "        # \"salary_in_usd\": FeatureSpace.float_normalized(),\n",
    "        \n",
    "        \"work_year\": FeatureSpace.float_normalized(),\n",
    "            },\n",
    "    # Specify feature cross with a custom crossing dim\n",
    "    crosses=[\n",
    "        FeatureSpace.cross(\n",
    "            feature_names=('employee_residence','employment_type'), # dims: 9 x 3 x 4 = 108 \n",
    "            crossing_dim=9*3*4),\n",
    "        FeatureSpace.cross(\n",
    "            feature_names=('company_location','company_size'), # 8 x 3 = 24\n",
    "            crossing_dim=8*3),\n",
    "        FeatureSpace.cross(\n",
    "            feature_names=('job_title','experience_level'), # 12 x 4 = 48\n",
    "            crossing_dim=12*4),\n",
    "    ],\n",
    "    output_mode=\"concat\",\n",
    ")\n",
    "# workclass has 7 unique values.\n",
    "# education has 16 unique values.\n",
    "# marital_status has 7 unique values.\n",
    "# occupation has 14 unique values.\n",
    "# relationship has 6 unique values.\n",
    "# race has 5 unique values.\n",
    "# sex has 2 unique values.\n",
    "# country has 41 unique values.\n",
    "\n",
    "# add explanation of this pre-processing here\n",
    "train_ds_with_no_labels = ds_train.map(lambda x, _: x)\n",
    "feature_space.adapt(train_ds_with_no_labels)\n",
    "\n",
    "def setup_embedding_from_crossing(feature_space, col_name):\n",
    "    # what the maximum integer value for this variable?\n",
    "    \n",
    "    # get the size of the feature\n",
    "    N = feature_space.crossers[col_name].num_bins\n",
    "    x = feature_space.crossers[col_name].output\n",
    "    \n",
    "    \n",
    "    # now use an embedding to deal with integers as if they were one hot encoded\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col_name+'_embed')(x)\n",
    "    \n",
    "    x = Flatten()(x) # get rid of that pesky extra dimension (for time of embedding)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d8163-7031-40e0-9460-af943a90debd",
   "metadata": {},
   "source": [
    "Thanks to dimensionality reduction, we reduced the number of parameters by about 7-fold. This reduction leaves us room to cross some of these features to build stronger connections in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a9bb6-f732-4a7a-85cc-887a95cc90f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_inputs = feature_space.get_inputs() # need to use unprocessed features here, to gain access to each output\n",
    "\n",
    "# we need to create separate lists for each branch\n",
    "crossed_outputs = []\n",
    "\n",
    "# for each crossed variable, make an embedding\n",
    "for col in feature_space.crossers.keys():\n",
    "    \n",
    "    x = setup_embedding_from_crossing(feature_space, col)\n",
    "    \n",
    "    # save these outputs in list to concatenate later\n",
    "    crossed_outputs.append(x)\n",
    "    \n",
    "\n",
    "# now concatenate the outputs and add a fully connected layer\n",
    "wide_branch = Concatenate(name='wide_concat')(crossed_outputs)\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "# for each numeric variable, just add it in after embedding\n",
    "for idx,col in enumerate(numeric_headers):\n",
    "    x = feature_space.preprocessors[col].output\n",
    "    # x = tf.cast(132,float) # cast an integer as a float here\n",
    "    all_deep_branch_outputs.append(x)\n",
    "    \n",
    "# for each categorical variable\n",
    "for col in categorical_headers:\n",
    "    \n",
    "    # get the output tensor from ebedding layer\n",
    "    x = setup_embedding_from_categorical(feature_space, col)\n",
    "    \n",
    "    # save these outputs in list to concatenate later\n",
    "    all_deep_branch_outputs.append(x)\n",
    "\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = Concatenate(name='embed_concat')(all_deep_branch_outputs)\n",
    "deep_branch = Dense(units=50,activation='relu', name='deep1')(deep_branch)\n",
    "deep_branch = Dense(units=25,activation='relu', name='deep2')(deep_branch)\n",
    "deep_branch = Dense(units=10,activation='relu', name='deep3')(deep_branch)\n",
    "    \n",
    "# merge the deep and wide branch\n",
    "final_branch = Concatenate(name='concat_deep_wide')([deep_branch, wide_branch])\n",
    "final_branch = Dense(units=1,activation='sigmoid',\n",
    "                     name='combined')(final_branch)\n",
    "\n",
    "training_model = keras.Model(inputs=dict_inputs, outputs=final_branch)\n",
    "training_model.compile(\n",
    "    optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "training_model.summary()\n",
    "\n",
    "plot_model(\n",
    "    training_model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e7cd86-e9ac-4baa-8337-478643a23cc4",
   "metadata": {},
   "source": [
    "### 1.3 Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69344c78-4383-43d8-99b1-61adbaa54b0e",
   "metadata": {},
   "source": [
    "##### Classification\n",
    "What are the implications of misclassifications?\n",
    "\n",
    "##### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9312f739-7c9a-4354-8390-cf863ef8a8a3",
   "metadata": {},
   "source": [
    "Stratified Shuffle Split:\n",
    "\n",
    "Use when you have an imbalanced dataset and want to combine the benefits of random shuffling with maintaining class distribution.\n",
    "Suitable for ensuring that each split is representative and for avoiding overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141cdd8d-e7c7-449e-8b10-1fcf01aa4f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Initialize StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform the split\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    train_data = data.iloc[train_index]\n",
    "    test_data = data.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e103d49-ea4f-491f-93fd-c7f355be0376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
